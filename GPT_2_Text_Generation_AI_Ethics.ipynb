{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT-2 Text-Generation AI Ethics",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ELehmann91/NLP/blob/master/GPT_2_Text_Generation_AI_Ethics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7LoMj4GA4n_",
        "colab_type": "text"
      },
      "source": [
        "#  Train a GPT-2 Text-Generating Model w/ GPU For Free \n",
        "\n",
        "by [Max Woolf](http://minimaxir.com)\n",
        "\n",
        "*Last updated: November 10th, 2019*\n",
        "\n",
        "Retrain an advanced text generating neural network on any text dataset **for free on a GPU using Collaboratory** using `gpt-2-simple`!\n",
        "\n",
        "For more about `gpt-2-simple`, you can visit [this GitHub repository](https://github.com/minimaxir/gpt-2-simple). You can also read my [blog post](https://minimaxir.com/2019/09/howto-gpt2/) for more information how to use this notebook!\n",
        "\n",
        "\n",
        "To get started:\n",
        "\n",
        "1. Copy this notebook to your Google Drive to keep it and save your changes. (File -> Save a Copy in Drive)\n",
        "2. Make sure you're running the notebook in Google Chrome.\n",
        "3. Run the cells below:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBkpRgBCBS2_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "0ccc4955-069c-439a-f0c2-be324e147776"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!pip install -q gpt-2-simple\n",
        "import gpt_2_simple as gpt2\n",
        "from datetime import datetime\n",
        "from google.colab import files"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bj2IJLHP3KwE",
        "colab_type": "text"
      },
      "source": [
        "## GPU\n",
        "\n",
        "Colaboratory uses either a Nvidia T4 GPU or an Nvidia K80 GPU. The T4 is slightly faster than the old K80 for training GPT-2, and has more memory allowing you to train the larger GPT-2 models and generate more text.\n",
        "\n",
        "You can verify which GPU is active by running the cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUmTooTW3osf",
        "colab_type": "code",
        "outputId": "aec3a6d3-5ea9-4bc2-de66-1bb376b015a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Jan  2 12:12:51 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.44       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wXB05bPDYxS",
        "colab_type": "text"
      },
      "source": [
        "## Downloading GPT-2\n",
        "\n",
        "If you're retraining a model on new text, you need to download the GPT-2 model first. \n",
        "\n",
        "There are three released sizes of GPT-2:\n",
        "\n",
        "* `124M` (default): the \"small\" model, 500MB on disk.\n",
        "* `355M`: the \"medium\" model, 1.5GB on disk.\n",
        "* `774M`: the \"large\" model, cannot currently be finetuned with Colaboratory but can be used to generate text from the pretrained model (see later in Notebook)\n",
        "* `1558M`: the \"extra large\", true model. Will not work if a K80 GPU is attached to the notebook. (like `774M`, it cannot be finetuned).\n",
        "\n",
        "Larger models have more knowledge, but take longer to finetune and longer to generate text. You can specify which base model to use by changing `model_name` in the cells below.\n",
        "\n",
        "The next cell downloads it from Google Cloud Storage and saves it in the Colaboratory VM at `/models/<model_name>`.\n",
        "\n",
        "This model isn't permanently saved in the Colaboratory VM; you'll have to redownload it if you want to retrain it at a later time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8wSlgXoDPCR",
        "colab_type": "code",
        "outputId": "e70eeda7-9a57-4ba4-cf24-0fff447b9422",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "gpt2.download_gpt2(model_name=\"124M\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 614Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 127Mit/s]                                                    \n",
            "Fetching hparams.json: 1.05Mit [00:00, 670Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:02, 211Mit/s]                                   \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 534Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 177Mit/s]                                                 \n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 189Mit/s]                                                       \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8KXuKWzQSsN",
        "colab_type": "text"
      },
      "source": [
        "## Mounting Google Drive\n",
        "\n",
        "The best way to get input text to-be-trained into the Colaboratory VM, and to get the trained model *out* of Colaboratory, is to route it through Google Drive *first*.\n",
        "\n",
        "Running this cell (which will only work in Colaboratory) will mount your personal Google Drive in the VM, which later cells can use to get data in/out. (it will ask for an auth code; that auth is not saved anywhere)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puq4iC6vUAHc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "ac278989-3348-45a5-8035-0b985d1ce314"
      },
      "source": [
        "gpt2.mount_gdrive()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT__brhBCvJu",
        "colab_type": "text"
      },
      "source": [
        "## Uploading a Text File to be Trained to Colaboratory\n",
        "\n",
        "In the Colaboratory Notebook sidebar on the left of the screen, select *Files*. From there you can upload files:\n",
        "\n",
        "![alt text](https://i.imgur.com/TGcZT4h.png)\n",
        "\n",
        "Upload **any smaller text file**  (<10 MB) and update the file name in the cell below, then run the cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OFnPCLADfll",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_name = \"ted.txt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzC3YfpOJnjj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "d1bbdef0-9d7c-4166-d736-3f7a7a2ed787"
      },
      "source": [
        "with open(file_name) as fp:\n",
        "  line = fp.readline()\n",
        "  print(line)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "So, I started my first job as a computer programmer in my very first year of college -- basically, as a teenager. Soon after I started working, writing software in a company, a manager who worked at the company came down to where I was, and he whispered to me, \"Can he tell if I'm lying?\" There was nobody else in the room. \"Can who tell if you're lying? And why are we whispering?\" The manager pointed at the computer in the room. \"Can he tell if I'm lying?\" Well, that manager was having an affair with the receptionist. And I was still a teenager. So I whisper-shouted back to him, \"Yes, the computer can tell if you're lying.\" Well, I laughed, but actually, the laugh's on me. Nowadays, there are computational systems that can suss out emotional states and even lying from processing human faces. Advertisers and even governments are very interested. I had become a computer programmer because I was one of those kids crazy about math and science. But somewhere along the line I'd learned about nuclear weapons, and I'd gotten really concerned with the ethics of science. I was troubled. However, because of family circumstances, I also needed to start working as soon as possible. So I thought to myself, hey, let me pick a technical field where I can get a job easily and where I don't have to deal with any troublesome questions of ethics. So I picked computers. Well, ha, ha, ha! All the laughs are on me. Nowadays, computer scientists are building platforms that control what a billion people see every day. They're developing cars that could decide who to run over. They're even building machines, weapons, that might kill human beings in war. It's ethics all the way down. Machine intelligence is here. We're now using computation to make all sort of decisions, but also new kinds of decisions. We're asking questions to computation that have no single right answers, that are subjective and open-ended and value-laden. We're asking questions like, \"Who should the company hire?\" \"Which update from which friend should you be shown?\" \"Which convict is more likely to reoffend?\" \"Which news item or movie should be recommended to people?\" Look, yes, we've been using computers for a while, but this is different. This is a historical twist, because we cannot anchor computation for such subjective decisions the way we can anchor computation for flying airplanes, building bridges, going to the moon. Are airplanes safer? Did the bridge sway and fall? There, we have agreed-upon, fairly clear benchmarks, and we have laws of nature to guide us. We have no such anchors and benchmarks for decisions in messy human affairs. To make things more complicated, our software is getting more powerful, but it's also getting less transparent and more complex. Recently, in the past decade, complex algorithms have made great strides. They can recognize human faces. They can decipher handwriting. They can detect credit card fraud and block spam and they can translate between languages. They can detect tumors in medical imaging. They can beat humans in chess and Go. Much of this progress comes from a method called \"machine learning.\" Machine learning is different than traditional programming, where you give the computer detailed, exact, painstaking instructions. It's more like you take the system and you feed it lots of data, including unstructured data, like the kind we generate in our digital lives. And the system learns by churning through this data. And also, crucially, these systems don't operate under a single-answer logic. They don't produce a simple answer; it's more probabilistic: \"This one is probably more like what you're looking for.\" Now, the upside is: this method is really powerful. The head of Google's AI systems called it, \"the unreasonable effectiveness of data.\" The downside is, we don't really understand what the system learned. In fact, that's its power. This is less like giving instructions to a computer; it's more like training a puppy-machine-creature we don't really understand or control. So this is our problem. It's a problem when this artificial intelligence system gets things wrong. It's also a problem when it gets things right, because we don't even know which is which when it's a subjective problem. We don't know what this thing is thinking. So, consider a hiring algorithm -- a system used to hire people, using machine-learning systems. Such a system would have been trained on previous employees' data and instructed to find and hire people like the existing high performers in the company. Sounds good. I once attended a conference that brought together human resources managers and executives, high-level people, using such systems in hiring. They were super excited. They thought that this would make hiring more objective, less biased, and give women and minorities a better shot against biased human managers. And look -- human hiring is biased. I know. I mean, in one of my early jobs as a programmer, my immediate manager would sometimes come down to where I was really early in the morning or really late in the afternoon, and she'd say, \"Zeynep, let's go to lunch!\" I'd be puzzled by the weird timing. It's 4pm. Lunch? I was broke, so free lunch. I always went. I later realized what was happening. My immediate managers had not confessed to their higher-ups that the programmer they hired for a serious job was a teen girl who wore jeans and sneakers to work. I was doing a good job, I just looked wrong and was the wrong age and gender. So hiring in a gender- and race-blind way certainly sounds good to me. But with these systems, it is more complicated, and here's why: Currently, computational systems can infer all sorts of things about you from your digital crumbs, even if you have not disclosed those things. They can infer your sexual orientation, your personality traits, your political leanings. They have predictive power with high levels of accuracy. Remember -- for things you haven't even disclosed. This is inference. I have a friend who developed such computational systems to predict the likelihood of clinical or postpartum depression from social media data. The results are impressive. Her system can predict the likelihood of depression months before the onset of any symptoms -- months before. No symptoms, there's prediction. She hopes it will be used for early intervention. Great! But now put this in the context of hiring. So at this human resources managers conference, I approached a high-level manager in a very large company, and I said to her, \"Look, what if, unbeknownst to you, your system is weeding out people with high future likelihood of depression? They're not depressed now, just maybe in the future, more likely. What if it's weeding out women more likely to be pregnant in the next year or two but aren't pregnant now? What if it's hiring aggressive people because that's your workplace culture?\" You can't tell this by looking at gender breakdowns. Those may be balanced. And since this is machine learning, not traditional coding, there is no variable there labeled \"higher risk of depression,\" \"higher risk of pregnancy,\" \"aggressive guy scale.\" Not only do you not know what your system is selecting on, you don't even know where to begin to look. It's a black box. It has predictive power, but you don't understand it. \"What safeguards,\" I asked, \"do you have to make sure that your black box isn't doing something shady?\" She looked at me as if I had just stepped on 10 puppy tails. She stared at me and she said, \"I don't want to hear another word about this.\" And she turned around and walked away. Mind you -- she wasn't rude. It was clearly: what I don't know isn't my problem, go away, death stare. Look, such a system may even be less biased than human managers in some ways. And it could make monetary sense. But it could also lead to a steady but stealthy shutting out of the job market of people with higher risk of depression. Is this the kind of society we want to build, without even knowing we've done this, because we turned decision-making to machines we don't totally understand? Another problem is this: these systems are often trained on data generated by our actions, human imprints. Well, they could just be reflecting our biases, and these systems could be picking up on our biases and amplifying them and showing them back to us, while we're telling ourselves, \"We're just doing objective, neutral computation.\" Researchers found that on Google, women are less likely than men to be shown job ads for high-paying jobs. And searching for African-American names is more likely to bring up ads suggesting criminal history, even when there is none. Such hidden biases and black-box algorithms that researchers uncover sometimes but sometimes we don't know, can have life-altering consequences. In Wisconsin, a defendant was sentenced to six years in prison for evading the police. You may not know this, but algorithms are increasingly used in parole and sentencing decisions. He wanted to know: How is this score calculated? It's a commercial black box. The company refused to have its algorithm be challenged in open court. But ProPublica, an investigative nonprofit, audited that very algorithm with what public data they could find, and found that its outcomes were biased and its predictive power was dismal, barely better than chance, and it was wrongly labeling black defendants as future criminals at twice the rate of white defendants. So, consider this case: This woman was late picking up her godsister from a school in Broward County, Florida, running down the street with a friend of hers. They spotted an unlocked kid's bike and a scooter on a porch and foolishly jumped on it. As they were speeding off, a woman came out and said, \"Hey! That's my kid's bike!\" They dropped it, they walked away, but they were arrested. She was wrong, she was foolish, but she was also just 18. She had a couple of juvenile misdemeanors. Meanwhile, that man had been arrested for shoplifting in Home Depot -- 85 dollars' worth of stuff, a similar petty crime. But he had two prior armed robbery convictions. But the algorithm scored her as high risk, and not him. Two years later, ProPublica found that she had not reoffended. It was just hard to get a job for her with her record. He, on the other hand, did reoffend and is now serving an eight-year prison term for a later crime. Clearly, we need to audit our black boxes and not have them have this kind of unchecked power.Audits are great and important, but they don't solve all our problems. Take Facebook's powerful news feed algorithm -- you know, the one that ranks everything and decides what to show you from all the friends and pages you follow. Should you be shown another baby picture? A sullen note from an acquaintance? An important but difficult news item? There's no right answer. Facebook optimizes for engagement on the site: likes, shares, comments. In August of 2014, protests broke out in Ferguson, Missouri, after the killing of an African-American teenager by a white police officer, under murky circumstances. The news of the protests was all over my algorithmically unfiltered Twitter feed, but nowhere on my Facebook. Was it my Facebook friends? I disabled Facebook's algorithm, which is hard because Facebook keeps wanting to make you come under the algorithm's control, and saw that my friends were talking about it. It's just that the algorithm wasn't showing it to me. I researched this and found this was a widespread problem. The story of Ferguson wasn't algorithm-friendly. It's not \"likable.\" Who's going to click on \"like?\" It's not even easy to comment on. Without likes and comments, the algorithm was likely showing it to even fewer people, so we didn't get to see this. Instead, that week, Facebook's algorithm highlighted this, which is the ALS Ice Bucket Challenge. Worthy cause; dump ice water, donate to charity, fine. But it was super algorithm-friendly. The machine made this decision for us. A very important but difficult conversation might have been smothered, had Facebook been the only channel. Now, finally, these systems can also be wrong in ways that don't resemble human systems. Do you guys remember Watson, IBM's machine-intelligence system that wiped the floor with human contestants on Jeopardy? It was a great player. But then, for Final Jeopardy, Watson was asked this question: \"Its largest airport is named for a World War II hero, its second-largest for a World War II battle.\" Chicago. The two humans got it right. Watson, on the other hand, answered \"Toronto\" -- for a US city category! The impressive system also made an error that a human would never make, a second-grader wouldn't make. Our machine intelligence can fail in ways that don't fit error patterns of humans, in ways we won't expect and be prepared for. It'd be lousy not to get a job one is qualified for, but it would triple suck if it was because of stack overflow in some subroutine. In May of 2010, a flash crash on Wall Street fueled by a feedback loop in Wall Street's \"sell\" algorithm wiped a trillion dollars of value in 36 minutes. I don't even want to think what \"error\" means in the context of lethal autonomous weapons. So yes, humans have always made biases. Decision makers and gatekeepers, in courts, in news, in war ... they make mistakes; but that's exactly my point. We cannot escape these difficult questions. We cannot outsource our responsibilities to machines. Artificial intelligence does not give us a \"Get out of ethics free\" card. Data scientist Fred Benenson calls this math-washing. We need the opposite. We need to cultivate algorithm suspicion, scrutiny and investigation. We need to make sure we have algorithmic accountability, auditing and meaningful transparency. We need to accept that bringing math and computation to messy, value-laden human affairs does not bring objectivity; rather, the complexity of human affairs invades the algorithms. Yes, we can and we should use computation to help us make better decisions. But we have to own up to our moral responsibility to judgment, and use algorithms within that framework, not as a means to abdicate and outsource our responsibilities to one another as human to human. Machine intelligence is here. That means we must hold on ever tighter to human values and human ethics.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeeSKtNWUedE",
        "colab_type": "text"
      },
      "source": [
        "If your text file is larger than 10MB, it is recommended to upload that file to Google Drive first, then copy that file from Google Drive to the Colaboratory VM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z6okFD8VKtS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2.copy_file_from_gdrive(file_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdpZQXknFNY3",
        "colab_type": "text"
      },
      "source": [
        "## Finetune GPT-2\n",
        "\n",
        "The next cell will start the actual finetuning of GPT-2. It creates a persistent TensorFlow session which stores the training config, then runs the training for the specified number of `steps`. (to have the finetuning run indefinitely, set `steps = -1`)\n",
        "\n",
        "The model checkpoints will be saved in `/checkpoint/run1` by default. The checkpoints are saved every 500 steps (can be changed) and when the cell is stopped.\n",
        "\n",
        "The training might time out after 4ish hours; make sure you end training and save the results so you don't lose them!\n",
        "\n",
        "**IMPORTANT NOTE:** If you want to rerun this cell, **restart the VM first** (Runtime -> Restart Runtime). You will need to rerun imports but not recopy files.\n",
        "\n",
        "Other optional-but-helpful parameters for `gpt2.finetune`:\n",
        "\n",
        "\n",
        "*  **`restore_from`**: Set to `fresh` to start training from the base GPT-2, or set to `latest` to restart training from an existing checkpoint.\n",
        "* **`sample_every`**: Number of steps to print example output\n",
        "* **`print_every`**: Number of steps to print training progress.\n",
        "* **`learning_rate`**:  Learning rate for the training. (default `1e-4`, can lower to `1e-5` if you have <1MB input data)\n",
        "*  **`run_name`**: subfolder within `checkpoint` to save the model. This is useful if you want to work with multiple models (will also need to specify  `run_name` when loading the model)\n",
        "* **`overwrite`**: Set to `True` if you want to continue finetuning an existing model (w/ `restore_from='latest'`) without creating duplicate copies. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeXshJM-Cuaf",
        "colab_type": "code",
        "outputId": "669d9500-d5c7-45c3-cae1-5b81d949b9eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 870
        }
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.finetune(sess,\n",
        "              dataset=file_name,\n",
        "              model_name='124M',\n",
        "              steps=100,\n",
        "              restore_from='fresh',\n",
        "              run_name='run1',\n",
        "              print_every=10,\n",
        "              sample_every=20,\n",
        "              save_every=50\n",
        "              )"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Loading checkpoint models/124M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|██████████| 1/1 [00:00<00:00,  4.28it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset has 20532 tokens\n",
            "Training...\n",
            "[10 | 18.48] loss=2.91 avg=2.91\n",
            "[20 | 30.99] loss=2.08 avg=2.49\n",
            "======== SAMPLE 1 ========\n",
            ", and that in the short run, those countries will become more successful economically, at the margin and morally, than we would have us believe. When we think about the global implications of our present thinking, it becomes evident why we should consider the current scenario in the circumstances.\n",
            "Posted by William Lane Craig at 17:11 AM\n",
            "I am not a Marxist, but I don't believe it is right to inflict pain or suffering without due process. Perhaps some will find that offensive. The choice that is made is up to the individual: a person should decide for themselves how much they like to live. I suspect that some will find that offensive -- and this is why the debate should focus on the ethical dimensions of such choices. But there is no guarantee that a society will accept such a society, or that such a society would accept non-discrimination. This may or may not be a sufficient answer. So I would ask that we draw the conclusions which are most conducive to allowing for the creation of an autonomous society in the context of an autonomous society operating within autonomous circumstances. If there is such a society, is there any criterion on which one might restrict one's ability to do what one is doing?\n",
            "If we do draw a parallel between the ethics of human-assisted assisted death and some other ethical principles that have emerged in recent centuries, let us draw one possible definition of what it should mean to be human:\n",
            "\"I am a human being … The Ethics of the Few.\" Since the mid-1980s, people across the developing world have come to accept the inevitability of human assisted death as an acceptable means of ending long-lasting human suffering. This has received strong support among human rights activists.\n",
            "We might call this the ethical imperative – we might call it the human imperative. To be sure, that term has been misconstrued by some. It comes from the Greek neinistion, which means 'to give up'. And while we sometimes believe that some moral principles are relevant to our own lives, our very conception of what those principles are ought to rest on the judgments of a person or a group of people. This person or a people discussion should focus on the principles that we hold to hold about living things we regard as morally essential to a successful society. The debate should focus on the ethical issues raised here. It will be important to understand precisely what the criteria really are for the recognition of the ethical value in assisted dying.\n",
            "The debate needs to be won. But we must win fast. Even as late as 2008, the debate is still very much in doubt about the values of euthanasia and assisted dying. Our goal should be to make it legal and to make sure that it is legal in all jurisdictions.\n",
            "One way of taking a position is that when it comes to human beings, we should be very sure. What we should avoid is assuming that all humans can, by some objective criterion, be assisted. That would be a huge misstep. Our goal should be to make it legal. We should make it a moral right, not a technical right.\n",
            "Obviously, it is not clear that we ought to follow in the footsteps of the European Commission or the European Court of Human Rights. And yet it seems likely that a number of human rights advocates (particularly those who support euthanasia and assisted dying) are likely to be present at this event. The debate will probably prove too important to miss – especially as the debate grows, as it should.\n",
            "There is something deeply wrong about thinking that some values will be relevant to our own lives, such as the autonomy which is enshrined in the Fourth Geneva Convention on Human Rights and the right of the children of war victims to education and health care. In this regard, this text must be understood in terms of a broader policy discussion which needs to be taking place.\n",
            "We do need to consider the ethical implications, not just in terms of the moral dimensions of assisted and in vitro fertilisation, but also in the implications for society as a whole.\n",
            "As soon as these are decided, then we should have a world without being judged. So I ask that we accept the position that we should make the decisions which are best for the good of society, and for humanity. Good, though we may all agree upon some values, these will no more be accepted as principles by all governments and social actors.\n",
            "This is a serious debate – one we should not leave to our children, grandchildren or great-grandchildren. It will be important to understand all the facts about human beings, particularly their moral status, which will be brought out in this present debate. It is not just a moral debate. It will take place in many dimensions. The debate could take place anywhere.\n",
            "Is assisted death a good option? Some people like to imagine themselves as human beings, rather than animals. Others believe in a future in which we will have no form of self or self-governing society. A third of ordinary people would like to see a super-stabilised society governed by fair labour and fair treatment. While\n",
            "\n",
            "[30 | 53.70] loss=1.73 avg=2.23\n",
            "[40 | 66.22] loss=1.06 avg=1.94\n",
            "======== SAMPLE 1 ========\n",
            " would, however, imply a moral relativism of superimpositions. It may, for instance, be possible to argue for a positive human ethical value system as a consequence of devising a non-discriminatory algorithm for selecting moral negatives for ethical judging. Another problem with considering moral relativism as grounded in moral relativism, more generally, is that some ethical principles are objectively moral— even if, for example, they are not concrete physical objects connected in a concrete way to moral obligations. 6 This possibility is detailed in the section on moral principles. For clarity, it will be assumed that the various ethics systems have the same moral status. Ethics are moral only insofar as they are grounded in moral principles or principles of non-discrimination. Sec 4.2.3. Standards and procedures 701 Aesthetics and the Philosophy of Science commons These ethical procedures specify how ethical cases should operate when used as standards for ethical verification. The procedure sets out a set of ethical criteria for the operation of computers, including how an ethical principle about qualia like in the present ethical principle might be applicable to ordinary moral procedure. The procedure specifies how ethical principles might be applied to relevant empirical cases. The procedure specifies how ethical principles might be formulated to handle contexts in which they are not true or false. Standards and procedures may be divided into 3 sub-communities. Each sub-communities of ethics are concerned with a different problem in the design of computers. They are divided into 3 sub-disciplines: quantitative: those who believe that the theory explaining the general behavior of animals is coherent quantitative: those who believe that the theory explaining the general behavior of plants is coherent quantitative/ for nonlinear reasons such as non-paradigm shift Another important category of ethical criteria is called normative design. This involves the requirement that the rules governing the application of those rules be consistent across ethical cases. This specification covers some of the issues that have special relevance to ethical verification. 3. The Ethics of Artificial General Intelligence There are currently two general artificial intelligence systems, a Shem and the “Machine. “Shem” is a classic artificial intelligence system from Stanford University in the United States. It is a design inspired by supercomputers but significantly modified for civilian use. Few computer systems currently exist that can interact in the same way with solid or liquid water; indeed, many systems such as solid-state microscopes and vacuum tubes rely on a variety of unrelated properties for their operation. But recently, with the advent of modern processing with higher dimensional objects, a number of novel properties have been developed that make Shem even more suitable for large-scale verification. Superintelligence involves introducing novel cognitive algorithms that interact mechanically with other cognitive algorithms to produce intelligent results. There are, of course, ethical issues that must be considered in the interpretation of these ethical developments. These issues are discussed in more detail in the section on ethical issues. A final branch of ethics that emerges from the discussion of ethics is an ethical substrate. This involves defining normative contexts in which ethical decision-making must take place; defining the normative context in which this decision must take place; and setting out the normative consequences for these decisions that would result from the normative contexts in which those decisions were taken. A substrate also involves defining possible future implications for ethical practice and the normative implications for future ethical contexts in which those implications might be derived. A second set of ethical issues arises when we consider the possibility that various cognitive algorithms could be modified to produce the same results. Since artificial intelligence is increasingly being developed as a means of self-monitoring and control, this possibility should be considered one of substantial historical urgency. In pursuing this agenda, we should consider how the methods by which artificial intelligence is being trained could be modified to produce different effects. We will also consider how artificial intelligence could be modified to vary self-distinguishability, and how such a modified version might be able to distinguish coherently between people with the same cognitive architecture and entities. A final set of ethical issues arises when we consider the potential consequences of systematically improving upon human capabilities at large. The ethical implications of AI for human well-being are well established. For example, Kant has observed that the human race is destined to repeat itself only if AI intelligences can create an equivalent social status among humans. This prediction has received surprisingly little attention. Attempts to re-assess the moral status of AI-assisted projects Agrarian, Bostrom and Eliezer Yudkowsky try to explain why human beings are so good at chess and how humans are better at math. The empirical nature of human capability is well established. For example, human beings are more than a bit clever at solving difficult problems; humans are at least as good at solving mundane social problems as we are. These are good reasons to believe that human beings are good at some important intelligibility problem; these are good reasons not to believe that human beings are good at some important intelligibility problem; these are better reasons not to believe that human beings are good at some important intelligent problem. These are good reasons\n",
            "\n",
            "[50 | 87.69] loss=0.82 avg=1.71\n",
            "Saving checkpoint/run1/model-50\n",
            "[60 | 103.03] loss=0.49 avg=1.50\n",
            "======== SAMPLE 1 ========\n",
            " Bluem, a member of a clan with a stronger genetic basis. She became the world's first black chess player. But the world got really good at black cards a couple of decades later, partly because of smart pointers, partly because Lewis Carroll had it in for us. Two millennia later, our black boxes are still impressive: we count chess as our god, Solow[1] Raúl, the white knight of Spain. She beat him! (Maybe that's why she goes to school?) But now black chess is king. When we learned this in high school, all of a sudden, we had a black deck. Somebody dropped by our house and said, \"What a great find!\" And we jumped up and down in laughter. It's the visual language of chess, because we counted moves on the board and compared them to gold and silver odds. That was black! So black cards make a huge difference to our world. Humans have been writing these kinds of powerful new facts about the world for centuries, but nobody really expected that to change a single bit. Now we know that sometimes, even within groups, there are free will rules that people disagree about, such as here and there being moral agents. In some respects, that's much the same as having animals be choosier about giving and receiving than people are. When AlphaGo tested its new strategy on a diorama of Boston for signs of life, it came across an image of a couple in their mid-thirties with their mug in the bank. The couple had just left a party and were searching the seas for something to eat. They spotted a rock called Trakhov Bay, which had recently been razed to the ground. They spotted a Zhao Long sofa and walked away, but not before the two men informed the couple of the decision. This brought them to this: When humans made chess stronger by recombining neurons in the striatum and similar brain areas, there was an increased chance that people would come into contact with the structure—the keyboard and mouse pad—of superobjects called the “Wall.” This was true, but it made no sense to the couple; why spend their money searching the lake for something to eat? The king of all superobjects was also the one who asked for it, so perhaps it made more sense to them to spend their money on a sofa with a king, rather than trying to deduce the structure of a wall buried deep within a sofa. We don’t know. We also don’t think stars have lots of ideas what their own internal layout isAll in all, the question remains: How well artificial intelligence scales up in real-world contexts? Even if human intelligence can learn to classify itself as a strategy optimiser (Sandberg, 2001) or a builder (Sandberg, 2001) and can classify itself as a way to better predict future events (Sandberg, 2006a), the answer is very much always somewhere in the low-to-mid range. So far, so good. Can artificial intelligence grow to become the dominant intelligence on the planet, safe from the usual human-superior-proliferation challenges of binary yes or no? That depends on how quickly artificial intelligence scales up in practice: early predictions of profound human problems were infrequent; as we saw in the movie AIs and Robots, machine learning seems to be increasing well into human-superior-proliferation years. A similar debate is still missing in the increasingly questions about contemporary AI. Are we ready for AI to take over our daily lives? Artificial intelligence is always a concern, but the evidence so far is very mixed. Few things seem more concerning to us than the prospect of creating machines that act independently of us—a bulldozer with no human operators, and no manager—than self-driving cars. It might seem premature to speculate what kind of society would such a society be based on driverless cars themselves, but one thing is for certain: society as a whole is very, very, very interested in machine intelligence. If AI becomes the norm and human beings are left to develop artificial intelligence systems in ways that resemble our own, society as a whole is going to look a lot more diverse and more diverse than it does right now. 4. Superintelligence. When human minds develop faster than their machines, they will become superintelligence. Were human minds to evolve at a accelerated pace, they would become technologically advanced. It would therefore not be surprising if advanced humans developed powerful supercomputers that would duplicate human-equivalent machine intelligence. One possibility is that some advanced civilization might develop an artificial intellect specifically for the purpose of inferring superintelligent behavior from data on other civilizations' human or computer members. One common view holds that such civilizations might be intelligence-brutally irresponsible, requiring widespread intervention by intelligent humans to sap their own intelligence. Another view holds that such civilizations might be intelligent but discretely and irrationally motivated, a scenario which raises the question whether such civilizations would be subject to the standard subhuman\n",
            "\n",
            "[70 | 124.73] loss=0.23 avg=1.31\n",
            "[80 | 137.21] loss=0.18 avg=1.17\n",
            "======== SAMPLE 1 ========\n",
            " it to a value with respect to the value of its individual member states . . . Our task is not necessarily to formulate algorithms that optimize one systematized around a fixed set of identity traits; but rather, to provide an environment in which one can envision and envision and envision and envision and envision and envision and envision and contemplate and contemplate and contemplate and contemplate and contemplate and contemplate and contemplate and contemplate Substrate mental architecture described above that inherits the specific properties of each individual brain and its associated computational circuitry. Our task as humans is to synthesize, sustain, and distribute intelligence among diverse neural populations, and to produce stable and effective moral and political outcomes across the many different brain populations. We deliver the relevant components of an effective agent onto a computer; we render services to a local population upon request; and we deliver normative moral advice to a global audience. Our computations generate normative values, and normative ethical advice, and normative ethical design principles, and normative principles of thumb for the various operating systems that control them. The moral design principles and the normative ethical design principles apply to all relevant parts of the operating system family of applications. As described above, the moral design principles and the normative ethical design principles apply only to the operating system kernel and their associated components. The components that manage and run the social world in which we live—the programmers, the assembled components, the server(s), the storage devices, the feeders, the routers, the toaster—are all users of some sort, and their actions have normative implications. A toaster cannot be an individual or a corporation; they all share the same moral code. But a toaster cannot be a person, or a corporation, or a toaster, or a computer or toaster; or a group of toasters, brains, factories, toasters—it is a toad—and it is a human beings right, under international law to have his or her own civilization challenged, defended, and built. The Principle of Obligatory Coexistence applies even if no other feature of the toaster interferes in its obtaining a contract. The Principle of Substantial Coexistence also takes account ofand, allowing for variation without loss of cognitive functionality. A toaster with the same cognitive functionality as a designer but without the toaster brain would still be a toaster, but the toaster would have to take on more responsibilities; the toaster would have to operate more autonomously, and so on. Even if a toaster differs from a programmer in certain ways, its ability to do what it does because of the toaster brain would be different than a toaster with no brain functionality. It would therefore be a complete surprise if a toaster with the same cognitive functionality as a programmer had the same implementation of a specific operation. It would therefore be a complete surprise to learn, if justifiable suspicion has not been established, that somewhere along the way in the evolution of human civilizations humans have been able to combine cognitive capabilities unrelated to one another significantly better than can modern humans. This would be true in at least some respects: some had a better hand, and some a poorer one. But it would be wrong to blame the a**stor’t for the problem, for it would be wrong to blame the toaster for any additional cognitive degradation caused by the toaster’s relatively minor differences from the programmers. Attempts to reason about this point are inapplicable to todays” territory, since the same person could produce an implementation of several different kinds of cognitively challenged math (an exact duplicate of the problem specification in the Java programming language), using the same implementation of the underlying system’s storage structures and the same serial number systems as the toasters. Attempts to explain away this (apparently unintentional) subtraction of human level cognitive load by stating that the programmers made the substantial design, rather than the subtle difference in the programmers’ implementation, is superficially sound. But it fails in completely explaining away the subtraction, if any, of human level cognitive load. On closer inspection, however, it becomes evident that the toaster’s design may have been influenced, at least in some ways, by the higher awareness scores a toaster has of having learned to Draw a Day School Bus. Substrate mental architecture thus serves as a model for future AI algorithms, assuming that similar visualizations of the same AI algorithm with the same neural circuitry will be sufficiently similar to produce a toaster AI algorithm that is similar to and devoid of any intrinsic mental bias.II. The Challenge Now that AI has become a reality, what criteria should AI engineers have for selecting the next AI system? That question remains a subject for another AI research paper, but the present work attempts to draw attention to one of the largest ethical problems in the artificial intelligence space: how we, the toasters, might decide whether to give or take our trusty computerized robot hand sanitizer? We set out to answer this very question by applying an evolutionary cognitive technology called predictive programming. Precisely\n",
            "\n",
            "[90 | 158.69] loss=0.08 avg=1.04\n",
            "[100 | 171.17] loss=0.09 avg=0.94\n",
            "Saving checkpoint/run1/model-100\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXSuTNERaw6K",
        "colab_type": "text"
      },
      "source": [
        "After the model is trained, you can copy the checkpoint folder to your own Google Drive.\n",
        "\n",
        "If you want to download it to your personal computer, it's strongly recommended you copy it there first, then download from Google Drive. The checkpoint folder is copied as a `.rar` compressed file; you can download it and uncompress it locally."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHdTL8NDbAh3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2.copy_checkpoint_to_gdrive(run_name='run1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQJgV_b4bmzd",
        "colab_type": "text"
      },
      "source": [
        "You're done! Feel free to go to the **Generate Text From The Trained Model** section to generate text based on your retrained model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pel-uBULXO2L",
        "colab_type": "text"
      },
      "source": [
        "## Load a Trained Model Checkpoint\n",
        "\n",
        "Running the next cell will copy the `.rar` checkpoint file from your Google Drive into the Colaboratory VM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCcx5u7sbPTD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2.copy_checkpoint_from_gdrive(run_name='run1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTa6zf3e_9gV",
        "colab_type": "text"
      },
      "source": [
        "The next cell will allow you to load the retrained model checkpoint + metadata necessary to generate text.\n",
        "\n",
        "**IMPORTANT NOTE:** If you want to rerun this cell, **restart the VM first** (Runtime -> Restart Runtime). You will need to rerun imports but not recopy files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fxL77nvAMAX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "beed6fb7-79c9-4e68-a870-98a1a3583007"
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.load_gpt2(sess, run_name='run1')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading checkpoint checkpoint/run1/model-100\n",
            "INFO:tensorflow:Restoring parameters from checkpoint/run1/model-100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClJwpF_ACONp",
        "colab_type": "text"
      },
      "source": [
        "## Generate Text From The Trained Model\n",
        "\n",
        "After you've trained the model or loaded a retrained model from checkpoint, you can now generate text. `generate` generates a single text from the loaded model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RNY6RBI9LmL",
        "colab_type": "code",
        "outputId": "29483abf-9bf5-4a30-8065-12816a035f66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "gpt2.generate(sess, run_name='run1')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trying to understand the minds behind such seemingly magical systems could give us insight into the nature of reality and the means by which that reality can be shaped. But that's another story.)\n",
            "One of the things we don't understand in this century is the nature of artificial minds. Machines don't just move boxes or do calculations; they also build and operate autonomously of their own free will. And since human beings are not made of silicon, we cannot see or hear their thoughts—we must rely on computers. Yet even if we could see or hear their thoughts, it would not be without its risks. Widespread adoption of artificial intelligence could bring with it severe consequences. The European Union is already preparing for a possible currency war, while the European Central Bank is considering whether to raise the interest rate it is setting itself. Even if we are serious about ending inequality, the insecurity of early retirement, and the prospect of a life of relative freedom, there is no guarantee that such a system will be able to fulfill all of our basic human needs. Globalize the job market and make it more transparent about who is and isn't able to obtain a job. Make it more easily accessible to people with higher education. Prohibit discrimination based on place of origin, gender, age, region, or even any other characteristic—no more \"equal opportunity\" under the US educational system than there is for \"equal opportunity\" in the countries with the \"diversity most conducive to human flourishing.\" Even these goals pose a security threat. Would a computer program be able to handle all the hard work and the scrutiny of a human engineer? Would it be able to? All of these goals may seem ambitious, but they are surely achievable in modern society. At least they are if we want to avoid the fate of job discrimination in software development. The prospect of creating computers that are as intelligent as human beings is not conducive to bringing about a democratic society. Vingeing will not bring about an end to discrimination; it will only bring about an end to the practice of treating people the way we would like to be treated. The same is true of giving legal status to practices that harm others. The procedure by which an order is executed is not by birth or by event—it's by design. By design, I do not mean that a computer program randomly selects an unenvisioned subject to serve as its own lawyer. It means that, unless there is a compelling governmental interest, the policymaking process will be dominated by the object of satisfying that interest. Randomness does not make a scientist crazy—it just means that the scientist will be less likely to produce results that would be likely to produce results that would be motivated by any other desire. The procedure by which an abortion is or was denied or a commons debate about gender roles is determined by how it is interpreted. Randomness does not make a programmer stupid; it just means that the programmer will be less likely to produce results that would be likely to produce results that would be motivated by any other desire.\n",
            "Randomness does not make a scientist stupid—it just means that the programmer will be less likely to produce results that would be likely to be motivated by any other desire. Responsibility. Computers can be counted on to do more than they can manage. Will they do it safely or will they do it for fun? Responsibility is a powerful feature of human reasoning. It means that it can be expressed in terms of a set of ethical principles: should a person be pregnant or not, she/it must provide for her or her children. Responsibility can be expressed in various manners, including the kind we use. Seductive principles, which are often reserved for persons with mental disabilities, may be suitable here. Controversial principles, which may be reserved for persons with other mental retardations, may be suitable here. The general rule is that when it comes to moral issues, the more controversial the claim. Aorishants will be glad to hear that there is a principle at odds with your moral status. They may be right, of course, but they are not the whole story. Legitimacy is not an option for many people with intellectual disabilities; many of our fellow humans are still unaware of the clear injustice of having moral status. But we need to accept that there are people who are smart and successful and do good things, and people who are stupid and do evil things, and so on and so on and so on and so on and so on and so on and so on and so on and so on and so on and so on and so on… and so on and so on and so on and so on and so on and so on and so on and so on and so on and so on and so on and so on and so on and so on and so on and so on and so on and so on and so on and so on and so on and so on and so on and so on and so on and so on and so on and so on and so on and so on and so on and so on and\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF4-PqF0Fl7R",
        "colab_type": "text"
      },
      "source": [
        "If you're creating an API based on your model and need to pass the generated text elsewhere, you can do `text = gpt2.generate(sess, return_as_list=True)[0]`\n",
        "\n",
        "You can also pass in a `prefix` to the generate function to force the text to start with a given character sequence and generate text from there (good if you add an indicator when the text starts).\n",
        "\n",
        "You can also generate multiple texts at a time by specifing `nsamples`. Unique to GPT-2, you can pass a `batch_size` to generate multiple samples in parallel, giving a massive speedup (in Colaboratory, set a maximum of 20 for `batch_size`).\n",
        "\n",
        "Other optional-but-helpful parameters for `gpt2.generate` and friends:\n",
        "\n",
        "*  **`length`**: Number of tokens to generate (default 1023, the maximum)\n",
        "* **`temperature`**: The higher the temperature, the crazier the text (default 0.7, recommended to keep between 0.7 and 1.0)\n",
        "* **`top_k`**: Limits the generated guesses to the top *k* guesses (default 0 which disables the behavior; if the generated output is super crazy, you may want to set `top_k=40`)\n",
        "* **`top_p`**: Nucleus sampling: limits the generated guesses to a cumulative probability. (gets good results on a dataset with `top_p=0.9`)\n",
        "* **`truncate`**: Truncates the input text until a given sequence, excluding that sequence (e.g. if `truncate='<|endoftext|>'`, the returned text will include everything before the first `<|endoftext|>`). It may be useful to combine this with a smaller `length` if the input texts are short.\n",
        "*  **`include_prefix`**: If using `truncate` and `include_prefix=False`, the specified `prefix` will not be included in the returned text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DKMc0fiej4N",
        "colab_type": "code",
        "outputId": "798aecff-e204-4b33-9cd8-6d8a0151c1f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "gpt2.generate(sess,\n",
        "              length=250,\n",
        "              temperature=0.9,\n",
        "              include_prefix=False,\n",
        "              truncate='<|endoftext|>',\n",
        "#              prefix=\"LORD\",\n",
        "              nsamples=5,\n",
        "              batch_size=5\n",
        "              )"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Imagine running a very large Python program on a very powerful computer. What happens if, for example, a powerful Python process executes a million times as fast as a simpler process? The resulting program would not be as autonomous as the Python that built its own diagnostic tools, or its own database of machine learning algorithms. In such a case, the ethics of running a very large Python process are debatable. Moreover, running a much larger process might produce results that differ little from the unintended consequence of anthropomorphizing a common set of mental states. Much like tasks in a production business can differ drastically from the outcomes we seek to optimize, a person's subjective well-being can and should differ drastically from job to job. It is therefore plausible that many of the same principles that led us to link subjective well-being to task success could be taken as foundational to the design of effective organizations. Theories of Subjective Well-Being can and should be reasserted in concrete settings. For instance, Feyerabend (2004) holds that in contexts of decision making characterized by uncertainty and ambiguity, such as those found in a legal complex, subjective well-being can be optimised by considering how well the people doing the thinking, or the avoiding,\n",
            "====================\n",
            "Saudi Arabia, the largest economy in the world, is under unprecedented economic and geopolitical pressure, and the world economy is in a tailspin, with all but the most powerful countries unable to exert influence through direct political engagement. The situation in Yemen, which Saudi Arabia captured from the internationally recognized government of Yemen in March 2014, has not been amenable to constructive direct talk since March of this year. While the situation in Mali has not been amenable to constructive direct discussion since March of 2014, the situation in Central African Republic is to be expected. Although the situation in Central African Republic is not amenable to constructive direct discussion since March of this year, the situation in Darfur, Sudan, is not amenable to constructive direct discussion since March of this year. While the situation in Darfur, which has not been amenable to direct discussion since March of this year, is indicative of a broader political and social instability, this section discusses the specific challenges and challenges that may lie ahead for the region. The Global Risks of Artificial Intelligence The technology is real, and it's getting better and better at an anplitude that is only just begun to usen us of a better world. But the underlying rationale behind such advanced intelligence is still a mystery\n",
            "====================\n",
            "Yes, sometimes the best practice is to steer clear of overt racism, classism, xenophobia and religious bias; rather, we should be careful not to lead by inspiring outright anti-racism, anti-immigrant, anti-immigrant, anti-immigrant positions in policymaking. The problem of implicit bias in decisionmaking is well-established.10 But a number of recent legal issues raise important questions about how we might constrain decisions about race. First, we might need to check whether a given law is implicit or explicit, in the sense of confirming or denying legal status. This would require that we understand the implicit nature of some decisions and the way we might interpret the way we signed them, and we should check with our legal advisors before we would be bound by a grant of statutory authorisation. This check should be based on a test we have developed (21 The Ethics of Legal Non-Discrimination in relation to Bias cases) that compares the likelihood that a given decision will be confirmed, rejected, or partially in place against other decisions in the situation. If the likelihood that a different decision will be confirmed, rejected, or completely in place is large (ie, as large as the probability that the two decisions will beuto beuto\n",
            "====================\n",
            "The parents of four-year-old Cassio Sanchez Sanchez who were sentenced to six years in prison for murdering his stepfather say their son will always be misunderstood and left unsupervised.\n",
            "“Trying to understand why the system does what it does is beyond my competence.””” The case against the world’s criminal class has grown threefold in the past decade, according to researchers at the University of Wisconsin–Madison. And while the statistics show that the general public is unaware of the criminal justice system's poor treatment of the past, the children’s generation is taking notice.\n",
            "“Trying to understand why the system does what it does is beyond my competence.””” Talk show host Stephen Colbert asked the question in an August 17 segment on Colbert and Friends, a local newspaper. host Stephen Colbert asked the question in an August 17 segment on Colbert and Friends, a local newspaper.\n",
            "\n",
            "\"Look,\" Colbert said, \"you have children. Who gives a hoot? Who’s picking the story?” The obvious next question would have been: \"Who picks the story?\" But the host went on: \"And who’s picking the\n",
            "====================\n",
            "I have written before about the necessity of parallel processing architectures in AI. One of the most important features of AI is the ability to imagine and imagine distant future directions. Our own species are possessed with the capacity for parallel processing: we write and communicate in many different languages, from our own languages to our own languages and hardware. But as we move across vast expanses of space and into new physical and biological boundaries, we're subject to a kind of self-modifying control problem. Should we make a move on a star-filled planet, or should we make a free kick in the planetary surface, to make a living? This sort of existential risk is not new. It is particularly relevant here given the way that AI algorithms have been deployed in many other instances in human history. As artificial intelligence advances, so too have human capabilities developed in ways that are not yet conducive to self-modifying control. Tasked experiments are always a good place to start. \"What if everything went well?\" you may ~ask',~' Fred,\" approaching the very question. \"What if everything went well?\" ~ Fred\n",
            "What if everything went well? Well, perhaps the results would count for something in the models. Perhaps our intuitions on such things\n",
            "====================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjjEN2Tafhl2",
        "colab_type": "text"
      },
      "source": [
        "For bulk generation, you can generate a large amount of text to a file and sort out the samples locally on your computer. The next cell will generate a generated text file with a unique timestamp.\n",
        "\n",
        "You can rerun the cells as many times as you want for even more generated texts!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fa6p6arifSL0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gen_file = 'gpt2_gentext_{:%Y%m%d_%H%M%S}.txt'.format(datetime.utcnow())\n",
        "\n",
        "gpt2.generate_to_file(sess,\n",
        "                      destination_path=gen_file,\n",
        "                      length=500,\n",
        "                      temperature=0.7,\n",
        "                      nsamples=100,\n",
        "                      batch_size=20\n",
        "                      )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-LRex8lfv1g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# may have to run twice to get file to download\n",
        "files.download(gen_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQAN3M6RT7Kj",
        "colab_type": "text"
      },
      "source": [
        "## Generate Text From The Pretrained Model\n",
        "\n",
        "If you want to generate text from the pretrained model, not a finetuned model, pass `model_name` to `gpt2.load_gpt2()` and `gpt2.generate()`.\n",
        "\n",
        "This is currently the only way to generate text from the 774M or 1558M models with this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsUd_jHgUZnD",
        "colab_type": "code",
        "outputId": "4e0c8a3f-3527-41c4-e3fe-3357f3f8f6c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "model_name = \"774M\"\n",
        "\n",
        "gpt2.download_gpt2(model_name=model_name)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 354Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 131Mit/s]                                                    \n",
            "Fetching hparams.json: 1.05Mit [00:00, 279Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 3.10Git [00:23, 131Mit/s]                                  \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 380Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 2.10Mit [00:00, 226Mit/s]                                                 \n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 199Mit/s]                                                       \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAe4NpKNUj2C",
        "colab_type": "code",
        "outputId": "b09bfe1d-2ff8-4b8a-fffb-273d28d5d4ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.load_gpt2(sess, model_name=model_name)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0828 18:37:58.571830 139905369159552 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading pretrained model models/774M/model.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xInIZKaU104",
        "colab_type": "code",
        "outputId": "56348e28-7d08-45e3-c859-f26c0efd066d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 797
        }
      },
      "source": [
        "gpt2.generate(sess,\n",
        "              model_name=model_name,\n",
        "              prefix=\"The secret of life is\",\n",
        "              length=100,\n",
        "              temperature=0.7,\n",
        "              top_p=0.9,\n",
        "              nsamples=5,\n",
        "              batch_size=5\n",
        "              )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The secret of life is that it's really easy to make it complicated,\" said Bill Nye, the host of the popular science show \"Bill Nye the Science Guy.\" \"And this is one of the reasons why we all need to be smarter about science, because we can't keep up with the amazing things that are going on all the time.\"\n",
            "\n",
            "While Nye is correct that \"everything that's going on all the time\" is making the world a better place, he misses the point. This is not\n",
            "====================\n",
            "The secret of life is in the rhythm of the universe. It's not a mystery. It's not a mystery to me. It's the nature of the universe. It's the beauty of the universe. It's the way the universe works. It's the way the universe is. It's the way the universe is going to work. It's the way the universe is. It's the way the universe is. It's the way the universe is. It's the way the universe is. It's the way\n",
            "====================\n",
            "The secret of life is in the universe.\n",
            "\n",
            "\n",
            "-\n",
            "\n",
            "The Red Devil\n",
            "\n",
            "It's the end of the world as we know it, and the only thing that can save us is a band of super-powered individuals known as the Red Devil.\n",
            "\n",
            "\n",
            "The Red Devil is a group of super-powered individuals who are seeking the secret of life and the only way they know how to do it is by taking on the roles of a variety of different super-powered individuals, each of which has their own\n",
            "====================\n",
            "The secret of life is in the mixing of the elements, and it is the mixing of the elements that makes life possible.\"\n",
            "\n",
            "But in the world of food science, the idea of a \"complex\" or \"complexity\" is almost entirely imaginary.\n",
            "\n",
            "As a scientist, I'm fascinated by the question of how life first began.\n",
            "\n",
            "It's the question that drives my work and the work of the scientists who work on it.\n",
            "\n",
            "My current research is exploring how microbes work in the first moments\n",
            "====================\n",
            "The secret of life is the journey of life, the search for the truth.\n",
            "\n",
            "4.4.2. The last thing you know\n",
            "\n",
            "There is nothing more important than the last thing you know.\n",
            "\n",
            "4.4.3. The little things that make all the difference\n",
            "\n",
            "The little things that make all the difference.\n",
            "\n",
            "4.4.4. The truth is the best teacher\n",
            "\n",
            "The truth is the best teacher.\n",
            "\n",
            "4.4.5. The truth is what\n",
            "====================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ig-KVgkCDCKD",
        "colab_type": "text"
      },
      "source": [
        "# Etcetera\n",
        "\n",
        "If the notebook has errors (e.g. GPU Sync Fail), force-kill the Colaboratory virtual machine and restart it with the command below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIHiVP53FnsX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kill -9 -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmTXWNUygS5E",
        "colab_type": "text"
      },
      "source": [
        "# LICENSE\n",
        "\n",
        "MIT License\n",
        "\n",
        "Copyright (c) 2019 Max Woolf\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE."
      ]
    }
  ]
}