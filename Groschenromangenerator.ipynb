{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ~~Master Thesis Generator~~ Groschenromangenerator\n",
    "This notebook uses text data from a famous german groschenromanautorin Carola Pigisch who publishes her lovely stories on http://groschenromanblog.de/ to generate our own groschenroman based on the style of Carola Pigisch. Lets have a look..\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper\n",
    "data_dir = 'data/Grandhotel Herz.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take her famous series Grandhotel Herz to let our algorythm learn from its passion.\n",
    "Lets have a first impression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'“Guten Morgen, Herr Ludenhoff. Wünschen wohl geruht zu haben, der Herr?” Johann stand stramm und lüf'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/Grandhotel Herz, Folge 1.txt', \"r\") as f:\n",
    "        data = f.read()\n",
    "data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Erneut schüttelte Sibille das Laken auf. Leicht wie eine Feder legte sich der kostbare Stoff auf die'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "bibliothek = []\n",
    "a =\"data\"\n",
    "for roman in os.listdir(a):\n",
    "    with open('data/'+roman, \"r\") as f:\n",
    "        data = f.read()\n",
    "        data = data.replace('Mitzi','Sibille')\n",
    "        data = data.replace('Max','Frank')\n",
    "        bibliothek.append(data)\n",
    "\n",
    "bibliothek[1][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "no names were changed, even if it looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176022"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ' '.join(roman for roman in bibliothek)\n",
    "with open('data/Grandhotel Herz.txt','w') as f: \n",
    "    data = f.write(text) \n",
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 7169\n",
      "Number of scenes: 245\n",
      "Average number of sentences in each scene: 2.7714285714285714\n",
      "Number of lines: 924\n",
      "Average number of words in each line: 30.713203463203463\n",
      "\n",
      "The sentences 0 to 2:\n",
      "“Guten Morgen, Herr Ludenhoff. Wünschen wohl geruht zu haben, der Herr?” Johann stand stramm und lüftete ganz leicht den schwarzen Zylinder auf seinem Kopf, als Frank Ludenhoff an ihm vorbeiging.\n",
      "“Guten Morgen, Johann. Danke der Nachfrage.” Frank Ludenhoff nickte Johan mit einem kaum merklichen Neigen des Kopfes zu. Er mochte den kleinen, rundlichen Portier, der schon seit mehr als 40 Jahren Morgen für Morgen pünktlich um halb acht an der goldenen Drehtür des Hotels stand und die Gäste empfing. “Heute kommt Gräfin Gurlitza, Johann, aber ich denke, Sie sind darauf eingestellt.”\n"
     ]
    }
   ],
   "source": [
    "view_sentence_range = (0, 2)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique words: {}'.format(len({word: None for word in text.split()})))\n",
    "scenes = text.split('\\n\\n')\n",
    "print('Number of scenes: {}'.format(len(scenes)))\n",
    "sentence_count_scene = [scene.count('\\n') for scene in scenes]\n",
    "print('Average number of sentences in each scene: {}'.format(np.average(sentence_count_scene)))\n",
    "\n",
    "sentences = [sentence for scene in scenes for sentence in scene.split('\\n')]\n",
    "print('Number of lines: {}'.format(len(sentences)))\n",
    "word_count_sentence = [len(sentence.split()) for sentence in sentences]\n",
    "print('Average number of words in each line: {}'.format(np.average(word_count_sentence)))\n",
    "\n",
    "print()\n",
    "print('The sentences {} to {}:'.format(*view_sentence_range))\n",
    "print('\\n'.join(text.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocessing Functions\n",
    "\n",
    "\n",
    "### Lookup Table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param text: The text of tv scripts split into words\n",
    "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    counts = Counter(text)\n",
    "    vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "    # Create dictionary that maps words to integers here\n",
    "    vocab_to_int = {word: ii for ii, word in enumerate(vocab)}\n",
    "    int_to_vocab = {i: word for word, i in vocab_to_int.items()}\n",
    "    \n",
    "    return (vocab_to_int, int_to_vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Punctuation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def token_lookup():\n",
    "    \"\"\"\n",
    "    Generate a dict to turn punctuation into a token.\n",
    "    :return: Tokenize dictionary where the key is the punctuation and the value is the token\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    tokenize = {\n",
    "        \".\": \"||period||\",\n",
    "        \",\": \"||comma||\",\n",
    "        \"\\\"\": \"||quotation||\",\n",
    "        \";\": \"||semicolon||\",\n",
    "        \"!\": \"||exclamation||\",\n",
    "        \"?\": \"||question||\",\n",
    "        \"(\": \"||left_parentheses||\",\n",
    "        \")\": \"||right_parentheses||\",\n",
    "        \"--\": \"||dash||\",\n",
    "        \"\\n\": \"||return||\",\n",
    "    }\n",
    "    return tokenize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the data and save it to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(data_dir, token_lookup, create_lookup_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import helper\n",
    "import numpy as np\n",
    "import problem_unittests as tests\n",
    "\n",
    "int_text, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: No GPU found. Please use a GPU to train your neural network. [ipykernel_launcher.py:14]\n"
     ]
    }
   ],
   "source": [
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_inputs():\n",
    "    \"\"\"\n",
    "    Create TF Placeholders for input, targets, and learning rate.\n",
    "    :return: Tuple (input, targets, learning rate)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    input = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    return (input, targets, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build RNN Cell and Initialize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_init_cell(batch_size, rnn_size):\n",
    "    \"\"\"\n",
    "    Create an RNN Cell and initialize it.\n",
    "    :param batch_size: Size of batches\n",
    "    :param rnn_size: Size of RNNs\n",
    "    :return: Tuple (cell, initialize state)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    lstm_basic = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([lstm_basic] * 1)\n",
    "    init_state = cell.zero_state(batch_size, tf.float32)\n",
    "    init_state = tf.identity(init_state, 'initial_state')\n",
    "\n",
    "    return (cell, init_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding\n",
    "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_embed(input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Create embedding for <input_data>.\n",
    "    :param input_data: TF placeholder for text input.\n",
    "    :param vocab_size: Number of words in vocabulary.\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Embedded input.\n",
    "    \"\"\"\n",
    "    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n",
    "    embed = tf.nn.embedding_lookup(embedding, input_data)\n",
    "    return embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build RNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def build_rnn(cell, inputs):\n",
    "    \"\"\"\n",
    "    Create a RNN using a RNN Cell\n",
    "    :param cell: RNN Cell\n",
    "    :param inputs: Input text data\n",
    "    :return: Tuple (Outputs, Final State)\n",
    "    \"\"\"\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32)\n",
    "    final_state = tf.identity(final_state, name='final_state')\n",
    "    return (outputs, final_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def build_nn(cell, rnn_size, input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Build part of the neural network\n",
    "    :param cell: RNN cell\n",
    "    :param rnn_size: Size of rnns\n",
    "    :param input_data: Input data\n",
    "    :param vocab_size: Vocabulary size\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Tuple (Logits, FinalState)\n",
    "    \"\"\"\n",
    "    embedding = get_embed(input_data, vocab_size, embed_dim)\n",
    "\n",
    "    outputs, final_state = build_rnn(cell, embedding)\n",
    "\n",
    "    logits = tf.contrib.layers.fully_connected(outputs, vocab_size,\n",
    "                                               activation_fn=None)\n",
    "    \n",
    "    return (logits, final_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_batches(int_text, batch_size, seq_length):\n",
    "    \"\"\"\n",
    "    Return batches of input and target\n",
    "    :param int_text: Text with the words replaced by their ids\n",
    "    :param batch_size: The size of batch\n",
    "    :param seq_length: The length of sequence\n",
    "    :return: Batches as a Numpy array\n",
    "    \"\"\"\n",
    "    int_text = np.array(int_text)\n",
    "    # Get the number of characters per batch and number of batches we can make\n",
    "    characters_per_batch = batch_size * seq_length\n",
    "    n_batches = len(int_text)//characters_per_batch\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    xdata = np.array(int_text[: n_batches * characters_per_batch])\n",
    "    ydata = np.array(int_text[1: n_batches * characters_per_batch + 1])\n",
    "    ydata[-1] = xdata[0]\n",
    "    \n",
    "    xdata = xdata.reshape((batch_size, -1))\n",
    "    ydata = ydata.reshape((batch_size, -1))\n",
    "    \n",
    "    # Reshape into n_seqs rows\n",
    "    inputs = np.split(xdata, n_batches, 1)\n",
    "    targets = np.split(ydata, n_batches, 1)\n",
    "    \n",
    "    return np.array(list(zip(inputs, targets)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Training\n",
    "### Hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "num_epochs = 100 #100\n",
    "# Batch Size\n",
    "batch_size = 128\n",
    "# RNN Size\n",
    "rnn_size = 256\n",
    "# Embedding Dimension Size\n",
    "embed_dim = 300\n",
    "# Sequence Length\n",
    "seq_length = 32\n",
    "# Learning Rate\n",
    "learning_rate = 0.01\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 8\n",
    "\n",
    "save_dir = './save'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib import seq2seq\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    vocab_size = len(int_to_vocab)\n",
    "    input_text, targets, lr = get_inputs()\n",
    "    input_data_shape = tf.shape(input_text)\n",
    "    cell, initial_state = get_init_cell(input_data_shape[0], rnn_size)\n",
    "    logits, final_state = build_nn(cell, rnn_size, input_text, vocab_size, embed_dim)\n",
    "\n",
    "    # Probabilities for generating words\n",
    "    probs = tf.nn.softmax(logits, name='probs')\n",
    "\n",
    "    # Loss function\n",
    "    cost = seq2seq.sequence_loss(\n",
    "        logits,\n",
    "        targets,\n",
    "        tf.ones([input_data_shape[0], input_data_shape[1]]))\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "    # Gradient Clipping\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch    0/8   train_loss = 8.508\n",
      "Epoch   1 Batch    0/8   train_loss = 6.410\n",
      "Epoch   2 Batch    0/8   train_loss = 5.806\n",
      "Epoch   3 Batch    0/8   train_loss = 5.305\n",
      "Epoch   4 Batch    0/8   train_loss = 4.862\n",
      "Epoch   5 Batch    0/8   train_loss = 4.439\n",
      "Epoch   6 Batch    0/8   train_loss = 4.039\n",
      "Epoch   7 Batch    0/8   train_loss = 3.661\n",
      "Epoch   8 Batch    0/8   train_loss = 3.327\n",
      "Epoch   9 Batch    0/8   train_loss = 3.093\n",
      "Epoch  10 Batch    0/8   train_loss = 2.878\n",
      "Epoch  11 Batch    0/8   train_loss = 2.667\n",
      "Epoch  12 Batch    0/8   train_loss = 2.477\n",
      "Epoch  13 Batch    0/8   train_loss = 2.265\n",
      "Epoch  14 Batch    0/8   train_loss = 2.103\n",
      "Epoch  15 Batch    0/8   train_loss = 1.928\n",
      "Epoch  16 Batch    0/8   train_loss = 1.793\n",
      "Epoch  17 Batch    0/8   train_loss = 1.641\n",
      "Epoch  18 Batch    0/8   train_loss = 1.531\n",
      "Epoch  19 Batch    0/8   train_loss = 1.428\n",
      "Epoch  20 Batch    0/8   train_loss = 1.357\n",
      "Epoch  21 Batch    0/8   train_loss = 1.294\n",
      "Epoch  22 Batch    0/8   train_loss = 1.208\n",
      "Epoch  23 Batch    0/8   train_loss = 1.136\n",
      "Epoch  24 Batch    0/8   train_loss = 1.162\n",
      "Epoch  25 Batch    0/8   train_loss = 1.019\n",
      "Epoch  26 Batch    0/8   train_loss = 0.949\n",
      "Epoch  27 Batch    0/8   train_loss = 0.813\n",
      "Epoch  28 Batch    0/8   train_loss = 0.738\n",
      "Epoch  29 Batch    0/8   train_loss = 0.654\n",
      "Epoch  30 Batch    0/8   train_loss = 0.629\n",
      "Epoch  31 Batch    0/8   train_loss = 0.569\n",
      "Epoch  32 Batch    0/8   train_loss = 0.543\n",
      "Epoch  33 Batch    0/8   train_loss = 0.517\n",
      "Epoch  34 Batch    0/8   train_loss = 0.472\n",
      "Epoch  35 Batch    0/8   train_loss = 0.436\n",
      "Epoch  36 Batch    0/8   train_loss = 0.411\n",
      "Epoch  37 Batch    0/8   train_loss = 0.378\n",
      "Epoch  38 Batch    0/8   train_loss = 0.341\n",
      "Epoch  39 Batch    0/8   train_loss = 0.324\n",
      "Epoch  40 Batch    0/8   train_loss = 0.297\n",
      "Epoch  41 Batch    0/8   train_loss = 0.277\n",
      "Epoch  42 Batch    0/8   train_loss = 0.260\n",
      "Epoch  43 Batch    0/8   train_loss = 0.244\n",
      "Epoch  44 Batch    0/8   train_loss = 0.236\n",
      "Epoch  45 Batch    0/8   train_loss = 0.221\n",
      "Epoch  46 Batch    0/8   train_loss = 0.216\n",
      "Epoch  47 Batch    0/8   train_loss = 0.215\n",
      "Epoch  48 Batch    0/8   train_loss = 0.197\n",
      "Epoch  49 Batch    0/8   train_loss = 0.197\n",
      "Epoch  50 Batch    0/8   train_loss = 0.184\n",
      "Epoch  51 Batch    0/8   train_loss = 0.168\n",
      "Epoch  52 Batch    0/8   train_loss = 0.169\n",
      "Epoch  53 Batch    0/8   train_loss = 0.152\n",
      "Epoch  54 Batch    0/8   train_loss = 0.145\n",
      "Epoch  55 Batch    0/8   train_loss = 0.136\n",
      "Epoch  56 Batch    0/8   train_loss = 0.130\n",
      "Epoch  57 Batch    0/8   train_loss = 0.123\n",
      "Epoch  58 Batch    0/8   train_loss = 0.121\n",
      "Epoch  59 Batch    0/8   train_loss = 0.116\n",
      "Epoch  60 Batch    0/8   train_loss = 0.111\n",
      "Epoch  61 Batch    0/8   train_loss = 0.110\n",
      "Epoch  62 Batch    0/8   train_loss = 0.108\n",
      "Epoch  63 Batch    0/8   train_loss = 0.103\n",
      "Epoch  64 Batch    0/8   train_loss = 0.103\n",
      "Epoch  65 Batch    0/8   train_loss = 0.097\n",
      "Epoch  66 Batch    0/8   train_loss = 0.099\n",
      "Epoch  67 Batch    0/8   train_loss = 0.095\n",
      "Epoch  68 Batch    0/8   train_loss = 0.092\n",
      "Epoch  69 Batch    0/8   train_loss = 0.092\n",
      "Epoch  70 Batch    0/8   train_loss = 0.087\n",
      "Epoch  71 Batch    0/8   train_loss = 0.086\n",
      "Epoch  72 Batch    0/8   train_loss = 0.083\n",
      "Epoch  73 Batch    0/8   train_loss = 0.081\n",
      "Epoch  74 Batch    0/8   train_loss = 0.079\n",
      "Epoch  75 Batch    0/8   train_loss = 0.079\n",
      "Epoch  76 Batch    0/8   train_loss = 0.077\n",
      "Epoch  77 Batch    0/8   train_loss = 0.076\n",
      "Epoch  78 Batch    0/8   train_loss = 0.076\n",
      "Epoch  79 Batch    0/8   train_loss = 0.075\n",
      "Epoch  80 Batch    0/8   train_loss = 0.074\n",
      "Epoch  81 Batch    0/8   train_loss = 0.074\n",
      "Epoch  82 Batch    0/8   train_loss = 0.073\n",
      "Epoch  83 Batch    0/8   train_loss = 0.073\n",
      "Epoch  84 Batch    0/8   train_loss = 0.072\n",
      "Epoch  85 Batch    0/8   train_loss = 0.072\n",
      "Epoch  86 Batch    0/8   train_loss = 0.071\n",
      "Epoch  87 Batch    0/8   train_loss = 0.071\n",
      "Epoch  88 Batch    0/8   train_loss = 0.070\n",
      "Epoch  89 Batch    0/8   train_loss = 0.070\n",
      "Epoch  90 Batch    0/8   train_loss = 0.070\n",
      "Epoch  91 Batch    0/8   train_loss = 0.069\n",
      "Epoch  92 Batch    0/8   train_loss = 0.069\n",
      "Epoch  93 Batch    0/8   train_loss = 0.069\n",
      "Epoch  94 Batch    0/8   train_loss = 0.069\n",
      "Epoch  95 Batch    0/8   train_loss = 0.068\n",
      "Epoch  96 Batch    0/8   train_loss = 0.068\n",
      "Epoch  97 Batch    0/8   train_loss = 0.068\n",
      "Epoch  98 Batch    0/8   train_loss = 0.068\n",
      "Epoch  99 Batch    0/8   train_loss = 0.067\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batches = get_batches(int_text, batch_size, seq_length)\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(num_epochs):\n",
    "        state = sess.run(initial_state, {input_text: batches[0][0]})\n",
    "\n",
    "        for batch_i, (x, y) in enumerate(batches):\n",
    "            feed = {\n",
    "                input_text: x,\n",
    "                targets: y,\n",
    "                initial_state: state,\n",
    "                lr: learning_rate}\n",
    "            train_loss, state, _ = sess.run([cost, final_state, train_op], feed)\n",
    "\n",
    "            # Show every <show_every_n_batches> batches\n",
    "            if (epoch_i * len(batches) + batch_i) % show_every_n_batches == 0:\n",
    "                print('Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    len(batches),\n",
    "                    train_loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_dir)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save parameters for checkpoint\n",
    "helper.save_params((seq_length, save_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import helper\n",
    "import problem_unittests as tests\n",
    "\n",
    "_, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()\n",
    "seq_length, load_dir = helper.load_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Generate Functions\n",
    "### Get Tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_tensors(loaded_graph):\n",
    "    \"\"\"\n",
    "    Get input, initial state, final state, and probabilities tensor from <loaded_graph>\n",
    "    :param loaded_graph: TensorFlow graph loaded from file\n",
    "    :return: Tuple (InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    InputTensor = loaded_graph.get_tensor_by_name(\"input:0\")\n",
    "    InitialStateTensor = loaded_graph.get_tensor_by_name(\"initial_state:0\")\n",
    "    FinalStateTensor = loaded_graph.get_tensor_by_name(\"final_state:0\")\n",
    "    ProbsTensor = loaded_graph.get_tensor_by_name(\"probs:0\")\n",
    "\n",
    "    return InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose Word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def pick_word(probabilities, int_to_vocab):\n",
    "    \"\"\"\n",
    "    Pick the next word in the generated text\n",
    "    :param probabilities: Probabilites of the next word\n",
    "    :param int_to_vocab: Dictionary of word ids as the keys and words as the values\n",
    "    :return: String of the predicted word\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    word_index = np.random.choice(len(int_to_vocab), 1, p=probabilities)\n",
    "    #print(word_index)\n",
    "    return int_to_vocab[word_index[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the Groschenroman\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n",
      "sandrine herrn da, dass der vater carl bezichtigt, die familie in verruf zu bringen und jeglichen kontakt zu seinem sohn verweigert. langsam und behutsam hatte die mutter wie ein blitz.\n",
      "“komm unbedingt ein! ”\n",
      "“du glaubst ja, freifrau, sibille! auf die floristin überhörte elisabeths arroganten und zurechtweisenden ton lief. es war schön, dich zu sehen. wie geht es ihnen? du hast mir nicht. ”\n",
      "sibille blieb der mund offen stehen. “meine mutter hieß auch lydia”, sagte er und sah seine augen, und nicht von nie. dann ist einfach doch nicht. ” er lachte. “auch für ehemalige….! ”\n",
      "kastlhuber verstand frank’ seitenhieb offenbar, denn sein mit einem kleinen hammer traktierte.\n",
      "“ach, ich bin nur da, herr ludenhoff. das dunkle haar, zum lässigen knoten aufgeschlungen, die feine nase und der knitze zug um. es war während meiner ausbildungszeit. er war journalist oder zumindest wollte er einer werden. ein klassischer, das ist monika eine andere frau oder aber waren an mich jetzt gehen? ”\n",
      "“prima, danke mein junge. die grippe habe ich gottlob gut überstanden. jetzt habe ich großen appetit auf ein herzhaftes frühstück. ” frank lachte sie. jetzt erst hatte er die echten freundin geworden. er war journalist oder zumindest wollte er einer werden. ein klassischer, das war es ein? ”\n",
      "\n",
      "sibille sah ihren vater an. sie wusste, dass er recht hatte. sie sah ihn nicht, sie für ihrn höchstens eine affäre frank wurde. sie war unfähig, einen klaren gedanken zu fassen, geschweige denn einen vernünftigen satz zu sagen. sie sah seine augen und gähnte sie. sie sah ihn an und ihr blick war eiskalt.\n",
      "“wenn du mich verlässt, erfährt du wirst eine echte adlige. da darfst du gewisse gesellschaftliche konventionen nicht außer acht lassen. gräfin zu jägermeinhaus nicht einzuladen, käme einem affront gleich. ich habe eben dem gast von 215 eine zeitung gebracht und dachte mir, ich schaue mal, ob sie heute dienst haben. ”\n",
      "\n",
      "sie bekamen einen schönen tisch am fenster und sebastiàn war wie immer frank in die augen, dachte er, die sofort in die augen. in erwartung des schlags duckte sich sibille, obwohl er völlig vergessen, dass er sich auf, drehte sich um. sein sehnsüchtiger blick ging ihr durch und durch. es drängte sie, ihn zu küssen und alle vorbehalte habe ich die kopien. noch nie im restaurant des ist. er war. aber insgeheim hatte er mehr rücksichtnahme von ihr erwartet. die aber nach dem moment war so noch einen kurzen minuten vorgestellt zeit, was der immer. ”\n",
      "“du hast angst,\n"
     ]
    }
   ],
   "source": [
    "gen_length = 500\n",
    "# homer_simpson, moe_szyslak, or Barney_Gumble\n",
    "prime_word = 'sandrine'\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    input_text, initial_state, final_state, probs = get_tensors(loaded_graph)\n",
    "\n",
    "    # Sentences generation setup\n",
    "    gen_sentences = [prime_word]\n",
    "    prev_state = sess.run(initial_state, {input_text: np.array([[1]])})\n",
    "\n",
    "    # Generate sentences\n",
    "    for n in range(gen_length):\n",
    "        # Dynamic Input\n",
    "        dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
    "        dyn_seq_length = len(dyn_input[0])\n",
    "\n",
    "        # Get Prediction\n",
    "        probabilities, prev_state = sess.run(\n",
    "            [probs, final_state],\n",
    "            {input_text: dyn_input, initial_state: prev_state})\n",
    "        \n",
    "        pred_word = pick_word(probabilities[dyn_seq_length-1], int_to_vocab)\n",
    "\n",
    "        gen_sentences.append(pred_word)\n",
    "    \n",
    "    # Remove tokens\n",
    "    tv_script = ' '.join(gen_sentences)\n",
    "    for key, token in token_dict.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        tv_script = tv_script.replace(' ' + token.lower(), key)\n",
    "    tv_script = tv_script.replace('\\n ', '\\n')\n",
    "    tv_script = tv_script.replace('( ', '(')\n",
    "        \n",
    "    print(tv_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THE END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
